## Write your short answer questions in this file.
## Simply replace the placeholders with your answers.

## Your code will go elsewhere (either in a ipynb or py file).
#  Where there are "YOUR CODE HERE" blocks, there exists a solution
#  by only editing those lines.  No need to edit anything else.

## Information Theory

# Part A

info_a1: -0.6780719051126379

info_a2: 4.321928094887363
info_a2_speculation: This kind of metric might be useful for entity identification. If two words have a very high probability of occuring next to one another and they are both proper nouns, then perhaps there is a higher chance of it being a two-word entity such as Washington Post. It might also be useful for recognizing idioms or common phrases.

# Part B

info_b1_1_128msg_num_bits: 7
info_b1_2_128msg_entropy: 7
info_b1_3_1024msg_num_bits: 10

info_b2:
- Second sentence
# (keep the correct answer)

info_b3:
- N(0, 1)
# (keep the correct answer)

info_c1: 0.3219280948873623
info_c2_1: 0.3219280948873623
info_c2_2: The values are the same
info_c3: If the label vector is one-hot, then you can simplify the Entropy calculation. Rather than summing p(x)*log_2*p(x) for each label, you can instead calculate p(x)*log_2*p(x) for only the "hot" label as the remaining labels will be zero.
info_c4: 0.0
info_c5: 1.7976931348623157e+308
info_c6: 1.7976931348623157e+308

## Dynamic Programming

# Part A
dp_a1: 1.6
dp_a2: It is slow because we re-calculate ways(n-2) through ways(n-(n-1))
dp_a3: 1.6

# Part B
dp_b1_A: 21 
dp_b1_B: 34
dp_b1_C: 35
dp_b2: yes
dp_b3: 2

# Part C
dp_c1: You only need to try each location of a single cut because profit_on_left_most is building a history of what your max potential profit would be for the left resulting part. This max potential profit already takes into account possible additional cuts to the left part. We could try cutting into three or more pieces but that would be duplicate work.
dp_c2: Because we iterate from 0 to n, we effectively cover all available splits. In the end, profit_on_left_most is a full table containing the max potential profit (which considers any number and series of cuts) of any length of cut bar less than n. As we iterate with j, if we discover that a left_most cut gives us a higher profit, then we save the higher profit.
dp_c3:
- 1
- 4
# (keep the correct answers)

# Part D
# For the next answers, write down the segmentation your best cuts with trace made.
# For example, if 'helloworldhowareyou' segmented into the words you'd expect, your
# answer would be as follows:
#
# dp_d_helloworldhowareyou:
# - hello
# - world
# - how
# - are
# - you

dp_d_helloworldhowareyou:
- hello
- world
- how
- are
- you

dp_d_downbythebay:
- down
- by
- the
- bay

dp_d_wikipediaisareallystrongresourceontheinternet:
- wikipedia
- is
- a
- really
- strong
- resource
- on
- the
- internet

# Part E
# (This section is optional, but if you want us to check your answers...)

dp_e1: 0
dp_e2: O(0)
dp_e3: 0
dp_e4: line goes here


## TensorFlow

# Part B

tf_b_W_shape: 10x1
tf_b_b_shape: 1x1

# Part C

tf_c_W_shape: 10x1
tf_c_b_shape: 1x1
tf_c_x_shape: 20x10
tf_c_z_shape: 10x1

# Part D

tf_d_y_hat_shape: 1x2
tf_d_y_hat_value: [0.689, 0.426]
tf_d_elementwise_description: Element-wise means that the function is applied to each element in the vector individually. This is similar to multiplying a matrix by a scalar.

# Part E
tf_e_W_shape: 10x20
tf_e_b_shape: 1x20
tf_e_x_shape: 1500x10
tf_e_z_shape: 1500x20
