{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup. Just run this cell once.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory Primer\n",
    "\n",
    "Here we'll explore a few basic concepts from [information theory](https://en.wikipedia.org/wiki/Information_theory) that are particularly relevant for this course. Information theory is a fairly broad subject, founded in the 1940s by [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), that gives a mathematical foundation for quantifying the communication of information. Shannon's original paper included, for example, the idea of the [bit](https://en.wikipedia.org/wiki/Bit), the minimal unit of information.\n",
    "\n",
    "It turned out that an information theoretic perspective is particularly convenient for understanding machine learning. So let's review the following concepts, which will appear throughout the course:\n",
    "\n",
    "* Mutual Information\n",
    "* Entropy\n",
    "* Cross-Entropy\n",
    "* KL Divergence\n",
    "\n",
    "You may have encountered several of these concepts in machine learning classes you've taken.  For similar reasons, they are important in NLP. We'll discuss them throughout the semester as they appear, but it is worth quickly going through the definitions once up front so they are fresh in your mind!\n",
    "\n",
    "Answer the questions below as you encounter them. There are a lot of questions, but almost all answers should be very short (less than one line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Mutual Information\n",
    "\n",
    "[Pointwise Mutual Information (PMI)](https://en.wikipedia.org/wiki/Pointwise_mutual_information) is a measure of how much knowing one outcome tells you about another.\n",
    "\n",
    "For example:  if I tell you it's raining, you'd probably guess it's cloudy.  But if I told you one die rolled 6, you couldn't guess what the other die rolled. Applied to language, if you mention the word \"rain\", I learn that you're more likely to also say \"wet\" or \"umbrella\" in the same context.\n",
    "\n",
    "The formula for PMI is as follows:\n",
    "\n",
    "$$\\text{PMI}(x, y) = \\log_2\\frac{p(x, y)}{p(x)\\ p(y)}$$\n",
    "\n",
    "In short, it measures the chance two outcomes tend to co-occur (the numerator) relative to the chance they would co-occur if they were independent events (the denominator). The $\\log_2$ makes it easier to reason about very large or very small values of this ratio - and let's us give it a unit: bits. We'll explain more about this below.\n",
    "\n",
    "\"Point-wise\" refers to the fact that we're picking single outcomes for \"x\" and \"y\" (i.e. x = \"raining\", y = \"cloudy\").  Without the point-wise (i.e. just [\"mutual information\"](https://en.wikipedia.org/wiki/Mutual_information)) refers to the average (expected value) point-wise mutual information between all possible assignments to x and y.\n",
    "\n",
    "If $X$ and $Y$ are independent, then $\\text{PMI}(x,y) = 0$ for all values of $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "[Entropy (Shannon entropy)](https://en.wikipedia.org/wiki/Shannon_entropy) is a notion of how \"uncertain\" the outcome of some experiment is.  The more uncertain - or the more spread out the distribution - the higher the entropy. *(Aside: some of you may have encountered \"entropy\" in a thermodynamics class.  Entropy as we're defining it here is conceptually similar - a notion of the amount of disorder in a system, and the concepts are closely related.)*\n",
    "\n",
    "Mathematically, for a (discrete) random variable $X$,\n",
    "\n",
    "$$\\text{Entropy}(X) = H(X) = -\\Sigma_x\\ p(x) \\log_2 p(x)$$\n",
    "\n",
    "*(We take $0 \\times log_2(0) = 0$, although in the \"real world\" probabilities are rarely equal to 0.)*\n",
    "\n",
    "Or, in English: imagine you have some probability distribution over a discrete set of events $X$.  Loop over each event and take the product of the probability of that event and the base-2 log of the probability.  Or, put another way, find the expected value $E[-\\log_2 p(x)]$ for this probability distribution.\n",
    "\n",
    "You have to admit, it's a bit of an odd calculation if you've never seen it before.  Why $\\log_2$?  Why is there a negative sign?\n",
    "\n",
    "Before we explain exactly what's going on, let's play with the equation a little.\n",
    "\n",
    "Let's compute the entropy of the result of a coin flip whose probability of heads is $P(\\text{heads}) = p$\n",
    "\n",
    "Fill in the code for BinaryEntropy below to calculate the entropy given a probability of heads, p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XLogX(x):\n",
    "    \"\"\"Returns x * log2(x).\"\"\"\n",
    "    return np.nan_to_num(x * np.log2(x))\n",
    "\n",
    "def BinaryEntropy(p):\n",
    "    \"\"\"Compute the entropy of a coin toss with P(heads) = p.\"\"\"\n",
    "    #### YOUR CODE HERE ####\n",
    "    return -XLogX(p) - XLogX(1 - p)\n",
    "    #### END YOUR CODE ####\n",
    "\n",
    "# Let's try running it for p = 0.  This means the coin always comes up \"tails\".\n",
    "# We expect that the entropy of this is 0 as there is no uncertainty about the outcome.\n",
    "assert 0.0 == BinaryEntropy(0)\n",
    "\n",
    "# We expect p = 0.5 to be as uncertain as it gets.  There's no good prior guess\n",
    "# as to which of heads or tails the coin is going to come down on.\n",
    "# As a result, we expect this to be bigger than p=0 above, but also bigger than any\n",
    "# other value of p.\n",
    "assert BinaryEntropy(0.5) > BinaryEntropy(0)\n",
    "assert BinaryEntropy(0.5) > BinaryEntropy(0.49)\n",
    "assert BinaryEntropy(0.5) > BinaryEntropy(0.51)\n",
    "\n",
    "# As it turns out the entropy at p=0.5 is 1.0.\n",
    "assert 1.0 == BinaryEntropy(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Entropy $H_2(X)$')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Poking at a couple of individual values is interesting, but we can also simply plot\n",
    "# entropy for all possible values of P(H).\n",
    "# As expected, the curve is maximum at p = 0.5 when the outcome is most uncertain\n",
    "# and decreases to 0 as either heads or tails becomes a certainty.\n",
    "p_of_heads = np.arange(0, 1.01, 0.01);\n",
    "plt.plot(p_of_heads, BinaryEntropy(p_of_heads))\n",
    "plt.xlabel('P(X = Heads)'); plt.ylabel('Entropy $H_2(X)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binary variable $x \\in \\{0,1\\}$ like our coin flip, the maximum entropy happens to be $H(X) = 1.0$. But don't be fooled by this - entropy is only bounded below (by 0), and can be arbitrarily large. We'll see this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Entropy: Sending Messages\n",
    "\n",
    "Imagine you want to send one of two messages to your friend, message **A** or message **B**. Imagine sending **A** and **B** were equally likely: $P(A) = P(B) = 0.5$, so you decide on the following code:\n",
    "\n",
    "```\n",
    "A -> 0\n",
    "B -> 1\n",
    "```\n",
    "Since there are only two options, a single bit will suffice. Note that 1 bit is equal to:\n",
    "\n",
    "$$ \\begin{eqnarray}\n",
    "1\\ \\text{bit} = -\\log_2\\frac{1}{2} & = & -\\frac{1}{2}\\log_2\\frac{1}{2} - \\frac{1}{2}\\log_2\\frac{1}{2} \\\\\n",
    "& = & - P(A) \\log_2 P(A) - P(B) \\log_2 P(B) \\\\\n",
    "& = & H_2(0.5) \\end{eqnarray}$$\n",
    "\n",
    "...which is exactly the entropy of the distribution of messages! Now imagine you want to send one of three messages $m \\sim M$:\n",
    "\n",
    "- **A** with $P(A) = 0.5$\n",
    "- **B** with $P(B) = 0.25$\n",
    "- **C** with $P(C) = 0.25$\n",
    "\n",
    "Since **A** is sent more often, we might want to give it a shorter code to save bandwidth. So we could try:\n",
    "\n",
    "```\n",
    "A -> 0\n",
    "B -> 10\n",
    "C -> 11\n",
    "```\n",
    "\n",
    "*Aside: note that this code is uniquely decodable left-to-right! This is known as a [Prefix code](https://en.wikipedia.org/wiki/Prefix_code).*\n",
    "\n",
    "How many bits does this code use, on average? Let's see:\n",
    "\n",
    "$$ 0.5\\times1\\ \\text{bit} + 0.25\\times2\\ \\text{bits} + 0.25\\times2\\ \\text{bits} = 1.5\\ \\text{bits} $$\n",
    "\n",
    "Which we note is once again equal to the entropy of the distribution:\n",
    "\n",
    "$$ H(M) = -0.5\\log_2(0.5) - 0.25\\log_2(0.25) - 0.25\\log_2(0.25) = 1.5\\ \\text{bits}$$\n",
    "\n",
    "It turns out that this code is optimal, and in general the entropy $H(M)$ is the fewest number of bits on average that _any_ code can use to send messages from the distribution $M$.\\* If we take bits to mean information, then the entropy is the _minimum_ amount of information needed (on average) to uniquely encode messages $m \\sim M$!\n",
    "\n",
    "As such, we often think of entropy as the **information content** of the distribution. It can be a counterintuitive way of thinking about it: a uniform distribution has high entropy, but doesn't tell us very much about what the value of a sample will be! But it means that when we do get a sample, it may contain a lot of information indeed.\n",
    "\n",
    "We often also think of entropy as how \"spread out\" a distribution is. In the extreme case, a distribution like:\n",
    "- **A** with $P(A) = 1.0$\n",
    "- **B** with $P(B) = 0.0$\n",
    "- **C** with $P(C) = 0.0$\n",
    "would require zero bits to specify, since all the mass is at one point A, so we already know what any message will be! So the entropy here would be 0. But if the mass is more spread out - like a uniform distribution, or, in continuous space, a very broad Gaussian - then many messages become equally likely, and so we need more bits (on average) to describe one.\n",
    "\n",
    "\\* This result is known as [Shannon's source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem), which states that an optimal code can be constructed by using $- \\log_2 p(m)$ bits for each specific message $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy\n",
    "\n",
    "What if we get the code wrong? Suppose we have a finite sample of messages (introducing some _variance_), and we train a machine learning model (introducing some _bias_) to estimate the true probabilities. Let's call the predicted distribution $Q(X)$.\n",
    "\n",
    "Now we generate a code based on $Q(X)$, and use it to encode real messages (which come from $P(X)$). How many bits do we use, on average?\n",
    "\n",
    "If we design an optimal code for $Q$, we use $-\\log_2 Q(x)$ bits for message $x$. Then we average this over $x \\sim P$ to get:\n",
    "\n",
    "$$ \\text{CE}(P,Q) = \\sum_{x} - P(x) \\log_2 Q(x) = \\mathrm{E}_{x \\sim P(x)}\\left[ -\\log_2 Q(x) \\right] $$\n",
    "\n",
    "Since we \"crossed\" the code from $Q$ and used it on $P$, this is known as the [**cross-entropy**](https://en.wikipedia.org/wiki/Cross_entropy). Note that $ \\text{CE}(P,Q) \\ge H(P) $, because we know that the code trained on $Q$ can't possibly be better than the optimal code on $P$ itself! \n",
    "\n",
    "In the form above, this is the most commonly used loss function in machine learning. In unsupervised learning (density estimation), we use it exactly as-is, with $x$ as the data. In supervised learning, we take the random variable to be the label $y$, and take our distributions to be conditional ones: $P(y\\ |\\ x)$ and $Q(y\\ |\\ x)$:\n",
    "$$ \\text{CE}(P,Q)(x) = \\sum_{y'} -P(y'\\ |\\ x) \\log_2 Q(y'\\ |\\ x) $$\n",
    "It's common to average over $x$ and to approximate $P(y\\ |\\ x)$ with discrete samples $(x,y)$ from a test set $T$, in which case we get:\n",
    "\n",
    "$$ \\text{CE}(P,Q) \n",
    "\\approx \\frac{1}{|T|} \\sum_{(x,y) \\in T} \\sum_{y'} -\\mathbb{1}[y = y'] \\log_2 Q(y'\\ |\\ x) \n",
    "= \\frac{1}{|T|} \\sum_{(x,y) \\in T} -\\log_2 Q(y\\ |\\ x) \n",
    "$$\n",
    "\n",
    "We'll commonly also write this using natural logarithms, but you can always convert between the two by the formula $ \\log_2(x) = \\log_2(e) \\cdot \\ln(x) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence\n",
    "\n",
    "We learned that we use the word \"cross-entropy\" to refer to the average number of bits we need if we design our code with $Q(X)$ (i.e. our model) in mind but end up sending with probability $P(X)$ (i.e. the test set).  Similarly, \"entropy\" is the average bits we need if we design with the right distribution in mind.\n",
    "\n",
    "We don't yet have a name for the difference between these quantities (i.e. the size of the *penalty* for using the wrong distribution to optimize our code).  That difference is known as the [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), or KL divergence for short.\n",
    "\n",
    "It is a measure of how different two probability distributions are.  The more $Q$ differs from $P$, the worse the penalty would be, and thus the higher the KL divergence.\n",
    "\n",
    "That is,\n",
    "$$ D_{KL}(P\\ ||\\ Q) = CE(P, Q) - H(P)$$\n",
    "\n",
    "From a machine learning perspective, the KL divergence measures the \"avoidable\" error - when our model is perfect (i.e. the *distribution* $\\hat{P}(y\\ |\\ x_i) = P(y\\ |\\ x_i)$, the KL divergence goes to zero. In general, the cross-entropy loss - and prediction accuracy - will not be zero, but will be equal to the entropy $H(P)$. This \"unavoidable\" error is the [Bayes error rate](https://en.wikipedia.org/wiki/Bayes_error_rate) for the underlying task.\n",
    "\n",
    "**Important note:** we're taking the true distribution $y_i = P(y\\ |\\ x_i) = [1, 0, 0, 0]$ in this example to be one-hot (i.e. all mass on one value). This is different from the one-hot encoding we use for training labels, which are _observations_ (single values) _sampled_ from some true distribution $y_i^\\text{obs} \\sim P(y\\ |\\ x_i)$. In that case, one-hot encoding is just a convenient shortcut. In general - and particularly in NLP - we'll have some label uncertainty, and the true distribution $P(y\\ |\\ x_i)$ will be spread among potentially many possibilities.\n",
    "\n",
    "One final observation:  cross-entropy is not symmetric (think about how $P$ and $Q$ appear in the formula to understand why).  As a result KL Divergence isn't symmetric either (if you want to prove this to yourself substitute in for CE and E in the equation above and simplify).  So while KL divergence is a measure of similarity, it's useful to keep in mind that $D_{KL}(P\\ ||\\ Q) \\ne D_{KL}(Q\\ ||\\ P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises (12 points)\n",
    "\n",
    "### A. Pointwise Mutual Information\n",
    "\n",
    "1. If $P(\\text{rainy}, \\text{cloudy})= 0.1$, $P(\\text{rainy}) = 0.2$ and $P(\\text{cloudy}) = 0.8$, what is $\\text{PMI}(\\text{rainy}, \\text{cloudy})$?\n",
    "2. Imagine $x$ is some word in a sentence, and $y$ is the next word in the sentence.  Imagine $P(\\text{washington})=0.01$, $P(\\text{post}) = 0.01$, and $P(\\text{washington}, \\text{post}) = 0.002$.  What is $\\text{PMI}(\\text{washington}, \\text{post})$?  Speculate why this kind of metric might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1.\n",
      "\n",
      "PMI(rainy, cloudy) =  -0.6780719051126379\n",
      "\n",
      "a2.\n",
      "\n",
      "PMI(washington, post) =  4.321928094887363\n",
      "This kind of metric might be useful for entity identification. If two words have a very high probability of occuring next to one another and they are both proper nouns, then perhaps there is a higher chance of it being a two-word entity such as Washington Post. It might also be useful for recognizing idioms or common phrases.\n"
     ]
    }
   ],
   "source": [
    "def pmi(p_xy, p_x, p_y):\n",
    "    \"\"\"\n",
    "    Implements PMI function from PMI section above\n",
    "    \"\"\"\n",
    "    return np.log2(\n",
    "        np.nan_to_num(p_xy) / (np.nan_to_num(p_x) * np.nan_to_num(p_y))\n",
    "    )\n",
    "     \n",
    "print('a1.\\n')\n",
    "print(\"PMI(rainy, cloudy) = \", pmi(p_xy=0.1, p_x=0.2, p_y=0.8))\n",
    "\n",
    "print('\\na2.\\n')\n",
    "print(\"PMI(washington, post) = \", pmi(p_xy=0.002, p_x=0.01, p_y=0.01))\n",
    "print('This kind of metric might be useful for entity identification. If two words have a very high probability of occuring '\n",
    "      'next to one another and they are both proper nouns, then perhaps there is a higher chance of it being a two-word '\n",
    "      'entity such as Washington Post. It might also be useful for recognizing idioms or common phrases.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### B. Entropy\n",
    "\n",
    "1. What if you had 128 messages, sending each with a probability of 1/128?  What's the expected number of bits?  What is the entropy of this distribution? What about 1024 messages each with probability 1/1024?\n",
    "2. Consider the following sentences, and a hypothetical distribution over words that could fill in the blank:  \n",
    "`\"How much wood could a _____ chuck if a woodchuck could chuck wood?\"`  \n",
    "`\"Hi, my name is _____.\"`  \n",
    "Which blank has higher entropy?\n",
    "3. Consider two normal (Gaussian) distributions: $x \\sim \\mathcal{N}(0,1)$ and $y \\sim \\mathcal{N}(7,0.5)$. Which variable has higher entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "\n",
      "128 messages\n",
      "\tExpected number of bits:  7.0\n",
      "\tEntropy:  7.0\n",
      "1024 messages\n",
      "\tExpected number of bits:  10.0\n",
      "\tEntropy:  10.0\n",
      "\n",
      "2.\n",
      "\n",
      "Blank #2 because there are a very limited number of words that could fill the \"___chuck\" gap while the space of possible names for blank #2 is quite large\n",
      "\n",
      "3.\n",
      "\n",
      "x∼N(0,1) has higher entropy because its variance is greater and thus the space of values covered by the distribution is larger.\n"
     ]
    }
   ],
   "source": [
    "def bit_code_length(num_messages):\n",
    "    \"\"\"\n",
    "    Computes how many bits required to represent the provided number of messages\n",
    "    \n",
    "    (only really works when num_messages %% 2 == 0)\n",
    "    \"\"\"\n",
    "    return np.log2(num_messages)\n",
    "\n",
    "def expected_num_bits(num_messages, prob_of_each):\n",
    "    \"\"\"\n",
    "    Calculates expected number of bits needed for size of <num_messages> given each has the same probability of <prob_of_each>\n",
    "    \"\"\"\n",
    "    length = bit_code_length(num_messages)\n",
    "    return (prob_of_each * length) * num_messages\n",
    "\n",
    "def entropy(num_messages, prob_of_each):\n",
    "    \"\"\"\n",
    "    Calculates entropy for size of <num_messages> given each has the same probability of <prob_of_each>\n",
    "    \"\"\"\n",
    "    return np.sum(num_messages * [-XLogX(prob_of_each)])\n",
    "\n",
    "def b1(n, p):\n",
    "    print('{0} messages'.format(n))\n",
    "    print('\\tExpected number of bits: ', expected_num_bits(n, p))\n",
    "    print('\\tEntropy: ', entropy(n, p))\n",
    "    return\n",
    "\n",
    "print('1.\\n')\n",
    "b1(128, 1 / 128)\n",
    "b1(1024, 1 / 1024)\n",
    "\n",
    "print('\\n2.\\n')\n",
    "\n",
    "print('Blank #2 because there are a very limited number of words that could fill the \"___chuck\" '\n",
    "      'gap while the space of possible names for blank #2 is quite large')\n",
    "\n",
    "print('\\n3.\\n')\n",
    "\n",
    "print('x∼N(0,1) has higher entropy because its variance is greater and thus the space of values '\n",
    "      'covered by the distribution is larger.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Cross-Entropy and KL Divergence\n",
    "\n",
    "For the following questions, imagine you have a classification problem over four labels, $\\{0,1,2,3\\}$.  For some example $x_i$, the correct label is class $0$.  That is, our true distribution is $y_i = P(y\\ |\\ x_i) = [1, 0, 0, 0]$.  Your model generates this probability distribution over the classes: $\\hat{y}_i = \\hat{P}(y\\ |\\ x_i) = [0.8, 0.1, 0.05, 0.05]$.\n",
    "\n",
    "1.  Compute $\\text{CrossEntropy}(y, \\hat{y})$.\n",
    "2.  Find $D_{KL}(y\\ ||\\ \\hat{y})$.  Compare it to your answer to (c1).\n",
    "3.  When the label vector is \"one-hot\" as it is in this case (i.e. only a single category has any probability mass), do you actually need to compute everything?  Describe the simplification.\n",
    "4.  What would $\\text{CrossEntropy}(y, \\hat{y})$ be if your model assigned all probability mass to the correct class (class 0)? (i.e. if $\\hat{y}_i = y_i = [1, 0, 0, 0]$)\n",
    "5.  What if the model assigned all probability mass to class 1 instead?\n",
    "6.  What if the model assigned $\\frac{1}{3}$ to each of classes 1, 2, and 3, and zero to class 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1. CrossEntropy(y, y_hat) =  0.3219280948873623\n",
      "c2. KL Divergence =  0.3219280948873623\n",
      "c3. If the label vector is one-hot, then you can simplify the Entropy calculation. Rather than summing p(x)*log_2*p(x) for each label, you can instead calculate p(x)*log_2*p(x) for only the \"hot\" label as the remaining labels will be zero.\n",
      "c4. CrossEntropy(y, y_hat) =  -0.0\n",
      "c5. CrossEntropy(y, y_hat) =  1.7976931348623157e+308\n",
      "c6. CrossEntropy(y, y_hat) =  1.7976931348623157e+308\n"
     ]
    }
   ],
   "source": [
    "y = np.array([1, 0, 0, 0])\n",
    "yhat = np.array([0.8, 0.1, 0.05, 0.05])\n",
    "yhat_c4 = np.array([1, 0, 0, 0])\n",
    "yhat_c5 = np.array([0, 1, 0, 0])\n",
    "yhat_c6 = np.array([0, 1/3, 1/3, 1/3])\n",
    "\n",
    "assert(sum(y) == 1)\n",
    "assert(sum(yhat) == 1)\n",
    "\n",
    "\n",
    "def XLogY(x, y):\n",
    "    \"\"\"Returns x * log2(y).\"\"\"\n",
    "    return np.nan_to_num(x * np.log2(y))\n",
    "\n",
    "\n",
    "def CrossEntropy(P, Q):\n",
    "    \"\"\"\n",
    "    Calculates CE(P,Q)\n",
    "    \n",
    "    Assumes inputs are numpy arrays\n",
    "    \"\"\"\n",
    "    return -np.sum(XLogY(P, Q))\n",
    "\n",
    "\n",
    "def entropy(m):\n",
    "    \"\"\"\n",
    "    Calculates entropy for of np.array(m)\n",
    "    \"\"\"\n",
    "    return -np.sum(XLogX(m))\n",
    "    \n",
    "    \n",
    "def KLDivergence(P, Q):\n",
    "    \"\"\"\n",
    "    Calculates CE(P, Q) - H(P)\n",
    "    \n",
    "    Assumes inputs are numpy arrays\n",
    "    \"\"\"\n",
    "    ce = CrossEntropy(P, Q)\n",
    "    h = entropy(P)\n",
    "    return ce - h\n",
    "\n",
    "\n",
    "print('c1. CrossEntropy(y, y_hat) = ', CrossEntropy(y, yhat))\n",
    "print('c2. KL Divergence = ', KLDivergence(y, yhat))\n",
    "print('c3. If the label vector is one-hot, then you can simplify the Entropy calculation. Rather than summing '\n",
    "      'p(x)*log_2*p(x) for each label, you can instead calculate p(x)*log_2*p(x) for only the \"hot\" label as '\n",
    "      'the remaining labels will be zero.')\n",
    "print('c4. CrossEntropy(y, y_hat) = ', CrossEntropy(y, yhat_c4))\n",
    "print('c5. CrossEntropy(y, y_hat) = ', CrossEntropy(y, yhat_c5))\n",
    "print('c6. CrossEntropy(y, y_hat) = ', CrossEntropy(y, yhat_c6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
