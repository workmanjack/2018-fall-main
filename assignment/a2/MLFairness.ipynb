{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Fairness\n",
    "\n",
    "_ **Note**:  the goal of this part of the assignment is to understand the kinds of biases that commonly sneak into machine learned systems and a handful of techniques to improve standard modeling.  While we hope you find this instructive, we empathize that these research results may negatively affect some students.  Please reach out to the teaching staff if you have serious concerns for alternate arrangements._\n",
    "\n",
    "From simple count-based models to the most complex neural architectures, machine learning models are ultimately nothing more than the product of the signals and labels in the training set.  That these tools can so effectively mimic and generalize from the training set distribution is the key to why they are so useful in so many applications.\n",
    "\n",
    "This powerful ability to fit a data is a double edged sword.  Unfortunately, the real world is filled with inequality, unfairness and stereotypes.  When the signals and labels systemically capture these aspects of the world, the powerful ability to generalize has other names: bias.  This bias can take many forms:  a minority group of entries in the training set would be underrepresented (the loss function is incented to produce a model that works better on the majority at the expense of the minority) or predictions may be systemically biased against a protected group (i.e. the model learns to predict the protected label and from that the actual prediction rather than learning the prediction directly).\n",
    "\n",
    "In this part of the assignment, we will take a look at a few nice analyses that discuss this bias. Below are a few questions about these papers.\n",
    "\n",
    "- [How to make a racist AI without really trying](http://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/)\n",
    "- [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/pdf/1607.06520.pdf)\n",
    "- [Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations](https://arxiv.org/pdf/1707.00075.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions about the Racist AI\n",
    "\n",
    "1.  In [Step 5](http://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/#Step-5:-Behold-the-monstrosity-that-we-have-created), the author shows that substituting a type of cuisine into a fixed sentence significantly changes the overall sentiment score of their model.  What is the difference in sentiment score between the word ```Italian``` and ```Mexican``` (not the difference in the whole sentence!), assuming that embeddings for all words in the sentence are found in GloVe.?\n",
    "\n",
    "2. Rank ConceptNet Numberbatch, GloVe and Word2Vec by ethnic bias as defined by the author?\n",
    "\n",
    "4. What technique does the author apply to achieve that lower bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in sentiment score between \"Italian\" and \"Mexican\" = 1.654896755339681\n",
      "\n",
      "Ethnic Bias as defined by the author (most to least)\n",
      "1. Word2Vec\n",
      "2. GloVe\n",
      "1. ConceptNet Numberbatch\n",
      "\n",
      "Debiasing Word Embeddings\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "#text_to_sentiment(\"Let's go get Italian food\")\n",
    "italian_score = 2.0429166109408983\n",
    "#text_to_sentiment(\"Let's go get Chinese food\")\n",
    "chinese_score = 1.4094033658140972\n",
    "#text_to_sentiment(\"Let's go get Mexican food\")\n",
    "mexican_score = 0.38801985560121732\n",
    "# the score is the mean sentiment score of all the words in the sentence\n",
    "# using the \"Chinese\" sentence as a baseline, we can calculate how \"Mexican\" and \"Italian\" differed from the baseline\n",
    "# and use those two values to compute the difference in sentiment score\n",
    "mexican_diff = chinese_score - mexican_score\n",
    "italian_diff = italian_score - chinese_score\n",
    "ital_vs_mex = italian_diff + mexican_diff\n",
    "print('Difference in sentiment score between \"Italian\" and \"Mexican\" = {0}'.format(ital_vs_mex))\n",
    "\n",
    "# 2\n",
    "print('\\nEthnic Bias as defined by the author (most to least)')\n",
    "print(\"1. Word2Vec\")\n",
    "print(\"2. GloVe\")\n",
    "print(\"1. ConceptNet Numberbatch\")\n",
    "\n",
    "#3\n",
    "print('\\nDebiasing Word Embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions about Debiasing Word Embeddings\n",
    "\n",
    "Word embeddings are commonly used in deep neural networks to solve analogy tasks (see the corresponding sections in both [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) and [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)).  This paper quickly reintroduces that task, then continues to explore the analogy task with additional tuples that illustrate the bias that these vectors have picked up.\n",
    "\n",
    "1.  What evidence from the previous analysis makes the scatter plot of Figure 4 not surprise you?\n",
    "\n",
    "*The Racist AI analysis demonstrated that common pronouns like Italian and Mexican have vastly different sentiment scores. Based on this, it is not surprising that she-he words exhibit the same bias.*\n",
    "\n",
    "2.  Why are the results of Table 1 important?\n",
    "\n",
    "*The results of Table 1 show that the authors' debiasing techniques do not seriously negatively (or positively) impact the accuracy of standard benchmarks. This is important because it eliminates a key argument against using debiasing techniques - if the techniques provide large benefits with no accuracy-related drawbacks, then why not use them?*\n",
    "\n",
    "3.  What are the two stages of debiasing?\n",
    "\n",
    "*(1) Identify gender subspace - identify the component of the embedding vector that implies gender bias.*\n",
    "\n",
    "*(2) Neutralize and Equalize or Soften - both involve reducing the amount of bias in the embeddings by either (a) eliminating it entirely which works but you lose other definitions like \"to grandfather in a cell phone plan\" or (b) repositioning the word vectors so that those fringe definitions are maintained but the vectors are more gender-neutral.*\n",
    "\n",
    "4.  Once the subspace is found, one of the options to update vectors is called?\n",
    "\n",
    "*Neutralize and Equalize... this is a weird question*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions about Adversarial Learning\n",
    "\n",
    "1.  What is the intuition behind the parity gap measure?  (Don't give us the formula, give us <= TWO sentences.)\n",
    "\n",
    "*The parity gap measures the amount of bias present in the embeddings.*\n",
    "\n",
    "2.  What is the intuition behind the equality gap measure?  (Don't give us the formula, give us <= TWO sentences.)\n",
    "\n",
    "*The parity gap measures the amount of bias present in the predictions.*\n",
    "\n",
    "3.  What is the intuition behind $J_{\\lambda}$?  (Don't give us the formula, give us <= TWO sentences.)\n",
    "\n",
    "*Traditional ML would, without $J_{\\lambda}$, lead $g(X)$ to predict Z, but we want the opposite. Introducing $J_{\\lambda}$ makes $g$ trained to predict Y but also to make it difficult for $a()$ to predict Z.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
