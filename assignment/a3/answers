# Instructions:
#
# label: {answer1|answer2}
#
# should be edited to read
#
# label: answer1
# OR
# label: answer2

#################################
#  NGrams                       #
#################################

# Part A
a_1_distribution: k / (k * |V|). The counts C_abc and C_ab for a new context will both be zero.
a_1_depends_on_k: The answer depends on k because if k was not there, a new context's probability would always be zero.
a_2_good_estimate: Better
a_3_pqba: k / (Cab + k * |V|)
a_4_which_context: Context (b)
a_5_which_should: Context (a)

# Part C
# c_1: include c1.png in this directory with your derivation.
c_2_which_case: b
c_2_why: Case (b) because the first term in P_kn(c|b) relies on counts and thus will be very small for (b) therefore the alpha term will need to be very large to compensate to achieve normalization. Case (a) does not need alpha to compensate because its count value is already very large. 

# Part E
e_1_average_count_per_trigram_ignoring_zero_counts: 0
e_1_average_count_per_trigram_including_zero_counts: 0
e_2_brown: {4|5}
e_2_wikipedia: {4|5}
e_3_realistic: {addk|kn|unsmoothed}

################################
# Neural Language Model        #
################################

a_1_cell_func: "h = ..."
a_1_parameters: something with Vs and Hs
a_2_embedding_parameters: ...
a_2_output_parameters: ...
a_3_single_target_word: ...
a_3_full_distribution_of_all_target_words: ...
a_4_with_sampled_softmax: Your answer
a_4_with_hierarchical_softmax: Your answer
a_5_slowest_part: {embed|recurrent|output}

c_1_explain_run_epoch: Your answer.

d_1_number_agreement: Your answer
d_2_semantic_agreement: Your answer
d_3_JJ_order: Your answer
