# Instructions:
#
# label: {answer1|answer2}
#
# should be edited to read
#
# label: answer1
# OR
# label: answer2

#################################
#  NGrams                       #
#################################

# Part A
a_1_distribution: k / (k * |V|). The counts C_abc and C_ab for a new context will both be zero.
a_1_depends_on_k: The answer depends on k because if k was not there, a new context's probability would always be zero.
a_2_good_estimate: Better
a_3_pqba: k / (Cab + k * |V|)
a_4_which_context: Context (b)
a_5_which_should: Context (a)

# Part C
# c_1: include c1.png in this directory with your derivation.
c_2_which_case: b
c_2_why: Case (b) because the first term in P_kn(c|b) relies on counts and thus will be very small for (b) therefore the alpha term will need to be very large to compensate to achieve normalization. Case (a) does not need alpha to compensate because its count value is already very large. 

# Part E
e_1_average_count_per_trigram_ignoring_zero_counts: 0.528437914
e_1_average_count_per_trigram_including_zero_counts: 1.43357e-08
e_2_brown: 4
e_2_wikipedia: 4
e_3_realistic: kn

################################
# Neural Language Model        #
################################

a_1_cell_func: "h = sigmoid(h^{i}h^{-1}+W_{x} x_{i}+b_{h})" where W_{x} is the associated weight vector for the hidden cell
a_1_parameters: H*H + H*V
a_2_embedding_parameters: V
a_2_output_parameters: V
a_3_single_target_word: O(2HV + 2H^2 + H)
a_3_full_distribution_of_all_target_words: O((2HV + 2H^2 + H) * sequence_length)
a_4_with_sampled_softmax: replace V with k
a_4_with_hierarchical_softmax: Assuming hierachical softmax has the same worst case as binary search, replace V with log(V)
a_5_slowest_part: recurrent

c_1_explain_run_epoch: "run_epoch" is responsible for executing a full cycle (either training or test) over the inputted data batch-by-batch while keeping track of the total loss (or "cost"). "batch_iterator" is responsible for partitioning the inputted data into uniform sized batches for the model to consume. "h" is the "memory" part of the model and is an input into the hidden layer. It is initialized according to the setup of our model at the beginning of each epoch, and is then modified in each subsequent inner loop as it "remembers" the previous tokens in the sequence.

d_1_number_agreement: The probabilities are very close, but "is" wins over "are" ("are" is grammatically correct). This might be because the RNN was trained on more examples with "is" following "and the girl" than "are". There might be less examples in the training set with nouns combined by "and".
d_2_semantic_agreement: For the peanut example, the vegetable option is more plausible according to the model. This might be because there are more examples in the training data of folks commenting on their favorite vegetable. For the hungry example, the eat option is more plausible according to the model. This is correct. I would expect the 3-gram and 5-gram models to perform worse at these examples, as in both the key token ("peanuts" in the first and "hungry" in the second) are outside of the context window of 3- and 5-gram models. Neither model seems capable of grasping the concept of semantics perfectly.
d_3_JJ_order: First you have to consider that the it is rare for three adjectives to be strung together in everyday English. This means that the training data probably had very few examples of this. Second, the RNN is probably influenced by the "I have lots of" sequence preceding the adjectives, whereas a trigram model with a small enough context window would not be. Third, "square" and "green" can also be nouns. If the RNN cannot identify them as adjectives in this example, then it would have a problem.
