{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Language Model\n",
    "\n",
    "This is the \"working notebook\", with skeleton code to load and train your model, as well as run unit tests. See [rnnlm-instructions.ipynb](rnnlm-instructions.ipynb) for the main writeup.\n",
    "\n",
    "Run the cell below to import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackcworkman/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm_test' from '/home/jackcworkman/w266/assignment/a3/lstm/rnnlm_test.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) RNNLM Inputs and Parameters  \n",
    "\n",
    "### Questions for Part (a)\n",
    "You should use big-O notation when appropriate (i.e. computing $\\exp(\\mathbf{v})$ for a vector $\\mathbf{v}$ of length $n$ is $O(n)$ operations).  Assume for problems a(1-5) that:   \n",
    "> -- Cell is one layer,  \n",
    "> -- the embedding feature length and hidden-layer feature lengths are both H, and   \n",
    "> -- ignore at the moment batch and max_time dimensions.  \n",
    "\n",
    "1. Let $\\text{CellFunc}$ be a simple RNN cell (see async Section 5.8). Write the cell equation in terms of nonlinearities and matrix multiplication. How many parameters (matrix or vector elements) are there for this cell, in terms of `V` and `H`?\n",
    "<p>\n",
    "    \n",
    "    $sigmoid(h^{i}h^{-1}+W_{x} x_{i}+b_{h})$ where $W_{x}$ are the associated weights for the RNN cell\n",
    "    \n",
    "    There are $H^2 + H*V$ parameters.\n",
    "\n",
    "    \n",
    "2. How many parameters are in the embedding layer? In the output layer? (By parameters, we mean total number of matrix elements across all train-able tensors. A $m \\times n$ matrix has $mn$ elements.)\n",
    "\n",
    "  Embedding layer: $V$ elements\n",
    "    \n",
    "  Output layer: $V$ elements\n",
    "\n",
    "<p>\n",
    "3. How many calculations (floating point operations) are required to compute $\\hat{P}(w^{(i+1)})$ for a given *single* target word $w^{(i+1)}$, assuming $w^{(i)}$ given and $h^{(i-1)}$ already computed? How about for *all* target words?\n",
    "    \n",
    "    Single target word: $H*V + (H-1)*V + H*H + (H-1)*H + H = O(2HV + 2H^2 + H)$\n",
    "    \n",
    "    All target words: $O((2HV + 2H^2 + H) * sequencelength)$\n",
    "    \n",
    "<p>\n",
    "4. How does your answer to 3. change if we approximate $\\hat{P}(w^{(i+1)})$ with a sampled softmax with $k$ samples? How about if we use a hierarchical softmax? (*Recall that hierarchical softmax makes a series of left/right decisions using a binary classifier $P_s(\\text{right}) = \\sigma(u_s \\cdot o^{(i)} + b_s)$ at each split $s$ in the tree.*)\n",
    "    \n",
    "    Sampled softmax with k samples: replace V with k\n",
    "    \n",
    "    Hierarchical softmax: Assuming hierachical softmax has the same worst case as binary search, replace V with log(V)\n",
    "    \n",
    "<p>\n",
    "5. If you have an LSTM with $H = 200$ and use sampled softmax with $k = 100$, what part of the network takes up the most computation time during training? (*Choose \"embedding layer\", \"recurrent layer\", or \"output layer\"*.)\n",
    "    \n",
    "    recurrent\n",
    "\n",
    "Note: for $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times l}$, computing the matrix product $AB$ takes $O(mnl)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the RNNLM\n",
    "\n",
    "### Aside: Shapes Review\n",
    "\n",
    "Before we start, let's review our understanding of the shapes involved in this assignment and how they manifest themselves in the TF API.\n",
    "\n",
    "As in the [instructions](rnnlm-instructions.ipynb) notebook, $w$ is a matrix of wordids with shape batch_size x max_time.  Passing this through the embedding layer, we retrieve the word embedding for each, resulting in $x$ having shape batch_size x max_time x embedding_dim.  I find it useful to draw this out on a piece of paper.  When you do, you should end up with a rectangular prism with batch_size height, max_time width and embedding_dim depth.  Many tensors in this assignment share this shape (e.g. $o$, the output from the LSTM, which represents the hidden layer going into the softmax to make a prediction at every time step in every batch).\n",
    "\n",
    "![Three Dimensional Shape](common_shape.png)\n",
    "\n",
    "Since batch size and sentence length are only resolved when we run the graph, we construct the placeholder using \"None\" in the dimensions we don't know.  The .shape property renders these as ?s.  This should be familiar to you from batch size handling in earlier assignments, only now there are two dimensions of variable length.\n",
    "\n",
    "See the next cell for a concrete example (though in practice, we'd use a TensorFlow variable that we can train for the embeddings rather than a static array).  Notice how the shape of x_val matches the shape described earlier in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordid placeholder shape: (?, ?)\n",
      "x shape: (?, ?, 3)\n",
      "Embeddings shape: (2, 4, 3)\n",
      "Embeddings value:\n",
      " [[[2 2 2]\n",
      "  [3 3 3]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "wordid_ph = tf.placeholder(tf.int32, shape=[None, None])\n",
    "embedding_matrix = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "x = tf.nn.embedding_lookup(embedding_matrix, wordid_ph)\n",
    "\n",
    "print('wordid placeholder shape:', wordid_ph.shape)\n",
    "print('x shape:', x.shape)\n",
    "\n",
    "sess = tf.Session()\n",
    "# Two sentences, each with four words.\n",
    "wordids = [[1, 2, 1, 2], [0, 0, 0, 0]]\n",
    "x_val = sess.run(x, feed_dict={wordid_ph: wordids})\n",
    "print('Embeddings shape:', x_val.shape)  # 2 sentences, 4 words, \n",
    "print('Embeddings value:\\n', x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implmenting the RNNLM\n",
    "\n",
    "In order to better manage the model parameters, we'll implement our RNNLM in the `RNNLM` class in `rnnlm.py`. We've given you a skeleton of starter code for this, but the bulk of the implementation is left to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/tmp/w266/a3_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will load your implementation, construct the graph, and write a logdir for TensorBoard. You can bring up TensorBoard with:\n",
    "```\n",
    "cd assignment/a3\n",
    "tensorboard --logdir /tmp/w266/a3_graph --port 6006\n",
    "```\n",
    "As usual, check http://localhost:6006/ and visit the \"Graphs\" tab to inspect your implementation. Remember, judicious use of `tf.name_scope()` and/or `tf.variable_scope()` will greatly improve the visualization, and make code easier to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a few unit tests below to verify some *very* basic properties of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnnlm_test.TestRNNLMTrain) ... ok\n",
      "test_shapes_sample (rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 2.009s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error messages are intentionally somewhat spare, and that passing tests are no guarantee of model correctness! Your best chance of success is through careful coding and understanding of how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Training your RNNLM (5 points)\n",
    "\n",
    "We'll give you data loader functions in **`utils.py`**. They work similarly to the loaders in the Week 5 notebook.\n",
    "\n",
    "Particularly, `utils.rnnlm_batch_generator` will return an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "For example, using a toy corpus:  \n",
    "*(Ignore the ugly formatter code.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Input words w:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Target words y:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"w_%d\" % d for d in range(w.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(w, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target words y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"y_%d\" % d for d in range(y.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(y, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data we feed to our model will be word indices, but the shape will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the `run_epoch` function\n",
    "We've given you some starter code for logging progress; fill this in with actual call(s) to `session.run` with the appropriate arguments to run a training step. \n",
    "\n",
    "Be sure to handle the initial state properly at the beginning of an epoch, and remember to carry over the final state from each batch and use it as the initial state for the next.\n",
    "\n",
    "**Note:** we provide a `train=True` flag to enable train mode. If `train=False`, this function can also be used for scoring the dataset - see `score_dataset()` below.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "1.  Explain what this function does.  Be sure to include the role of `batch_iterator` and what's going on with `h` in the inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        feed_dict = {\n",
    "            lm.input_w_: w,\n",
    "            lm.target_y_: y,\n",
    "            lm.initial_h_: h,\n",
    "            lm.learning_rate_: learning_rate,\n",
    "            lm.use_dropout_: use_dropout\n",
    "        }\n",
    "        ops = [loss, lm.final_h_, train_op]        \n",
    "        #### YOUR CODE HERE ####\n",
    "        # session.run(...) the ops with the feed_dict constructed above.\n",
    "        # Ensure \"cost\" becomes the value of \"loss\".\n",
    "        # Hint: see \"ops\" for other variables that need updating in this loop.\n",
    "        ops = session.run(ops, feed_dict=feed_dict)\n",
    "        cost = ops[0]\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cell below to verify your implementation of `run_epoch`, and to test your RNN on a (very simple) toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 52]: seen 2650 words at 2649.1 wps, loss = 1.290\n",
      "[batch 140]: seen 7050 words at 3512.9 wps, loss = 0.721\n",
      "[batch 228]: seen 11450 words at 3805.9 wps, loss = 0.524\n",
      "[batch 316]: seen 15850 words at 3944.5 wps, loss = 0.423\n",
      "[batch 404]: seen 20250 words at 4026.6 wps, loss = 0.364\n",
      "[batch 492]: seen 24650 words at 4082.0 wps, loss = 0.324\n",
      "[batch 579]: seen 29000 words at 4115.9 wps, loss = 0.293\n",
      "[batch 665]: seen 33300 words at 4132.4 wps, loss = 0.270\n",
      "[batch 753]: seen 37700 words at 4158.5 wps, loss = 0.252\n",
      "[batch 840]: seen 42050 words at 4174.5 wps, loss = 0.237\n",
      "[batch 928]: seen 46450 words at 4191.8 wps, loss = 0.225\n",
      "[batch 1015]: seen 50800 words at 4203.0 wps, loss = 0.214\n",
      "[batch 1103]: seen 55200 words at 4216.1 wps, loss = 0.205\n",
      "[batch 1191]: seen 59600 words at 4227.5 wps, loss = 0.196\n",
      "[batch 1278]: seen 63950 words at 4233.6 wps, loss = 0.189\n",
      "[batch 1365]: seen 68300 words at 4238.5 wps, loss = 0.183\n",
      "[batch 1454]: seen 72750 words at 4250.1 wps, loss = 0.178\n",
      "[batch 1542]: seen 77150 words at 4256.5 wps, loss = 0.173\n",
      "[batch 1630]: seen 81550 words at 4263.1 wps, loss = 0.168\n",
      "[batch 1717]: seen 85900 words at 4267.1 wps, loss = 0.164\n",
      "[batch 1806]: seen 90350 words at 4273.8 wps, loss = 0.160\n",
      "[batch 1893]: seen 94700 words at 4276.9 wps, loss = 0.157\n",
      "[batch 1979]: seen 99000 words at 4277.1 wps, loss = 0.154\n",
      "[batch 2066]: seen 103350 words at 4278.4 wps, loss = 0.152\n",
      "[batch 2154]: seen 107750 words at 4282.9 wps, loss = 0.149\n",
      "[batch 2242]: seen 112150 words at 4286.0 wps, loss = 0.146\n",
      "[batch 2330]: seen 116550 words at 4289.8 wps, loss = 0.144\n",
      "[batch 2417]: seen 120900 words at 4291.9 wps, loss = 0.142\n",
      "[batch 2505]: seen 125300 words at 4294.5 wps, loss = 0.140\n",
      "[batch 2592]: seen 129650 words at 4296.3 wps, loss = 0.138\n",
      "[batch 2678]: seen 133950 words at 4296.0 wps, loss = 0.136\n",
      "[batch 2767]: seen 138400 words at 4299.6 wps, loss = 0.134\n",
      "[batch 2856]: seen 142850 words at 4303.2 wps, loss = 0.132\n",
      "[batch 2944]: seen 147250 words at 4305.4 wps, loss = 0.130\n",
      "[batch 3034]: seen 151750 words at 4309.6 wps, loss = 0.129\n",
      "[batch 3119]: seen 156000 words at 4308.0 wps, loss = 0.127\n",
      "[batch 3207]: seen 160400 words at 4309.8 wps, loss = 0.125\n",
      "[batch 3296]: seen 164850 words at 4312.6 wps, loss = 0.124\n",
      "[batch 3386]: seen 169350 words at 4316.5 wps, loss = 0.122\n",
      "[batch 3474]: seen 173750 words at 4317.4 wps, loss = 0.121\n",
      "[batch 3563]: seen 178200 words at 4320.1 wps, loss = 0.120\n",
      "[batch 3651]: seen 182600 words at 4321.5 wps, loss = 0.118\n",
      "[batch 3740]: seen 187050 words at 4323.4 wps, loss = 0.117\n",
      "[batch 3827]: seen 191400 words at 4323.7 wps, loss = 0.116\n",
      "[batch 3918]: seen 195950 words at 4327.7 wps, loss = 0.115\n",
      "Train set: avg. loss: 0.000  (perplexity: 1.00)\n",
      "Test set: avg. loss: 0.000  (perplexity: 1.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 47.869s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as above, this is a *very* simple test case that does not guarantee model correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Training\n",
    "\n",
    "We'll give you the outline of the training procedure, but you'll need to fill in a call to your `run_epoch` function. \n",
    "\n",
    "At the end of training, we use a `tf.train.Saver` to save a copy of the model to `/tmp/w266/a3_model/rnnlm_trained`. You'll want to load this from disk to work on later parts of the assignment; see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters\n",
    "With a sampled softmax loss, the default hyperparameters should train 5 epochs in around 15 minutes on a single-core GCE instance, and reach a training set perplexity between 120-140.\n",
    "\n",
    "However, it's possible to do significantly better. Try experimenting with multiple RNN layers (`num_layers` > 1) or a larger hidden state - though you may also need to adjust the learning rate and number of epochs for a larger model.\n",
    "\n",
    "You can also experiment with a larger vocabulary. This will look worse for perplexity, but will be a better model overall as it won't treat so many words as `<unk>`.\n",
    "\n",
    "#### Notes on Speed\n",
    "\n",
    "To speed things up, you may want to re-start your GCE instance with more CPUs. Using a 16-core machine will train *very* quickly if using a sampled softmax lost, almost as fast as a GPU. (Because of the sequential nature of the model, GPUs aren't actually much faster than CPUs for training and running RNNs.) The training code will print the words-per-second processed; with the default settings on a single core, you can expect around 8000 WPS, or up to more than 25000 WPS on a fast multi-core machine.\n",
    "\n",
    "You might also want to modify the code below to only run score_dataset at the very end, after all epochs are completed. This will speed things up significantly, since `score_dataset` uses the full softmax loss - and so often can take longer than a whole training epoch!\n",
    "\n",
    "#### Submitting your model\n",
    "You should submit your trained model along with the assignment. Do:\n",
    "```\n",
    "cp /tmp/w266/a3_model/rnnlm_trained* .\n",
    "git add rnnlm_trained*\n",
    "git commit -m \"Adding trained model.\"\n",
    "```\n",
    "Unless you train a very large model, these files should be < 50 MB and no problem for git to handle. If you do also train a large model, please only submit the smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/jackcworkman/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 25\n",
    "batch_size = 50\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=200, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/w266/a3_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 6]: seen 8750 words at 8494.9 wps, loss = 7.413\n",
      "[batch 13]: seen 17500 words at 8361.1 wps, loss = 6.979\n",
      "[batch 20]: seen 26250 words at 8433.6 wps, loss = 6.663\n",
      "[batch 28]: seen 36250 words at 8543.8 wps, loss = 6.392\n",
      "[batch 36]: seen 46250 words at 8719.9 wps, loss = 6.191\n",
      "[batch 43]: seen 55000 words at 8713.0 wps, loss = 6.050\n",
      "[batch 51]: seen 65000 words at 8804.7 wps, loss = 5.914\n",
      "[batch 58]: seen 73750 words at 8711.6 wps, loss = 5.822\n",
      "[batch 66]: seen 83750 words at 8693.1 wps, loss = 5.729\n",
      "[batch 74]: seen 93750 words at 8745.7 wps, loss = 5.646\n",
      "[batch 81]: seen 102500 words at 8704.0 wps, loss = 5.580\n",
      "[batch 89]: seen 112500 words at 8700.6 wps, loss = 5.517\n",
      "[batch 97]: seen 122500 words at 8744.1 wps, loss = 5.463\n",
      "[batch 105]: seen 132500 words at 8794.0 wps, loss = 5.411\n",
      "[batch 113]: seen 142500 words at 8832.5 wps, loss = 5.366\n",
      "[batch 121]: seen 152500 words at 8883.5 wps, loss = 5.326\n",
      "[batch 129]: seen 162500 words at 8916.8 wps, loss = 5.288\n",
      "[batch 138]: seen 173750 words at 8982.2 wps, loss = 5.247\n",
      "[batch 147]: seen 185000 words at 9059.5 wps, loss = 5.210\n",
      "[batch 158]: seen 198750 words at 9260.0 wps, loss = 5.168\n",
      "[batch 169]: seen 212500 words at 9433.0 wps, loss = 5.133\n",
      "[batch 181]: seen 227500 words at 9636.7 wps, loss = 5.095\n",
      "[batch 192]: seen 241250 words at 9802.3 wps, loss = 5.065\n",
      "[batch 203]: seen 255000 words at 9951.8 wps, loss = 5.038\n",
      "[batch 214]: seen 268750 words at 10088.3 wps, loss = 5.012\n",
      "[batch 225]: seen 282500 words at 10214.0 wps, loss = 4.989\n",
      "[batch 236]: seen 296250 words at 10328.3 wps, loss = 4.965\n",
      "[batch 247]: seen 310000 words at 10423.8 wps, loss = 4.943\n",
      "[batch 259]: seen 325000 words at 10540.9 wps, loss = 4.920\n",
      "[batch 270]: seen 338750 words at 10632.5 wps, loss = 4.902\n",
      "[batch 282]: seen 353750 words at 10740.0 wps, loss = 4.881\n",
      "[batch 294]: seen 368750 words at 10845.3 wps, loss = 4.863\n",
      "[batch 306]: seen 383750 words at 10947.1 wps, loss = 4.844\n",
      "[batch 318]: seen 398750 words at 11041.3 wps, loss = 4.828\n",
      "[batch 330]: seen 413750 words at 11134.1 wps, loss = 4.810\n",
      "[batch 342]: seen 428750 words at 11214.1 wps, loss = 4.796\n",
      "[batch 354]: seen 443750 words at 11302.3 wps, loss = 4.781\n",
      "[batch 366]: seen 458750 words at 11380.9 wps, loss = 4.766\n",
      "[batch 378]: seen 473750 words at 11454.1 wps, loss = 4.752\n",
      "[batch 390]: seen 488750 words at 11520.6 wps, loss = 4.739\n",
      "[batch 402]: seen 503750 words at 11586.9 wps, loss = 4.726\n",
      "[batch 414]: seen 518750 words at 11646.6 wps, loss = 4.714\n",
      "[batch 426]: seen 533750 words at 11706.6 wps, loss = 4.701\n",
      "[batch 438]: seen 548750 words at 11765.5 wps, loss = 4.688\n",
      "[batch 450]: seen 563750 words at 11816.3 wps, loss = 4.676\n",
      "[batch 462]: seen 578750 words at 11870.7 wps, loss = 4.665\n",
      "[batch 474]: seen 593750 words at 11922.0 wps, loss = 4.654\n",
      "[batch 486]: seen 608750 words at 11967.9 wps, loss = 4.643\n",
      "[batch 498]: seen 623750 words at 12010.0 wps, loss = 4.633\n",
      "[batch 510]: seen 638750 words at 12047.1 wps, loss = 4.623\n",
      "[batch 522]: seen 653750 words at 12082.0 wps, loss = 4.614\n",
      "[batch 534]: seen 668750 words at 12126.4 wps, loss = 4.604\n",
      "[batch 546]: seen 683750 words at 12162.0 wps, loss = 4.594\n",
      "[batch 557]: seen 697500 words at 12186.5 wps, loss = 4.586\n",
      "[batch 569]: seen 712500 words at 12221.0 wps, loss = 4.577\n",
      "[batch 581]: seen 727500 words at 12258.5 wps, loss = 4.569\n",
      "[batch 593]: seen 742500 words at 12296.9 wps, loss = 4.560\n",
      "[batch 605]: seen 757500 words at 12333.0 wps, loss = 4.551\n",
      "[batch 617]: seen 772500 words at 12367.0 wps, loss = 4.544\n",
      "[batch 629]: seen 787500 words at 12397.5 wps, loss = 4.536\n",
      "[batch 641]: seen 802500 words at 12430.2 wps, loss = 4.528\n",
      "[batch 653]: seen 817500 words at 12457.3 wps, loss = 4.521\n",
      "[batch 665]: seen 832500 words at 12484.2 wps, loss = 4.514\n",
      "[batch 677]: seen 847500 words at 12515.0 wps, loss = 4.507\n",
      "[batch 689]: seen 862500 words at 12536.7 wps, loss = 4.501\n",
      "[batch 700]: seen 876250 words at 12549.1 wps, loss = 4.494\n",
      "[batch 712]: seen 891250 words at 12570.1 wps, loss = 4.488\n",
      "[batch 723]: seen 905000 words at 12585.8 wps, loss = 4.482\n",
      "[batch 735]: seen 920000 words at 12607.4 wps, loss = 4.476\n",
      "[batch 747]: seen 935000 words at 12627.0 wps, loss = 4.469\n",
      "[batch 759]: seen 950000 words at 12648.0 wps, loss = 4.464\n",
      "[batch 771]: seen 965000 words at 12667.3 wps, loss = 4.458\n",
      "Train set: avg. loss: 5.225  (perplexity: 185.78)\n",
      "Test set: avg. loss: 5.289  (perplexity: 198.15)\n",
      "[epoch 1] Completed in 0:02:24\n",
      "[epoch 2] Starting epoch 2\n",
      "[batch 11]: seen 15000 words at 14023.6 wps, loss = 7.111\n",
      "[batch 23]: seen 30000 words at 14014.1 wps, loss = 6.590\n",
      "[batch 35]: seen 45000 words at 14143.2 wps, loss = 6.232\n",
      "[batch 47]: seen 60000 words at 14146.7 wps, loss = 6.003\n",
      "[batch 59]: seen 75000 words at 14181.1 wps, loss = 5.828\n",
      "[batch 71]: seen 90000 words at 14162.4 wps, loss = 5.682\n",
      "[batch 82]: seen 103750 words at 14100.2 wps, loss = 5.576\n",
      "[batch 93]: seen 117500 words at 14054.2 wps, loss = 5.493\n",
      "[batch 105]: seen 132500 words at 14083.6 wps, loss = 5.409\n",
      "[batch 117]: seen 147500 words at 14151.1 wps, loss = 5.343\n",
      "[batch 129]: seen 162500 words at 14179.7 wps, loss = 5.286\n",
      "[batch 141]: seen 177500 words at 14232.1 wps, loss = 5.234\n",
      "[batch 153]: seen 192500 words at 14247.8 wps, loss = 5.187\n",
      "[batch 165]: seen 207500 words at 14268.9 wps, loss = 5.146\n",
      "[batch 177]: seen 222500 words at 14290.5 wps, loss = 5.109\n",
      "[batch 189]: seen 237500 words at 14283.0 wps, loss = 5.075\n",
      "[batch 201]: seen 252500 words at 14294.6 wps, loss = 5.042\n",
      "[batch 213]: seen 267500 words at 14308.2 wps, loss = 5.012\n",
      "[batch 225]: seen 282500 words at 14308.5 wps, loss = 4.983\n",
      "[batch 237]: seen 297500 words at 14288.2 wps, loss = 4.958\n",
      "[batch 249]: seen 312500 words at 14268.9 wps, loss = 4.934\n",
      "[batch 260]: seen 326250 words at 14241.3 wps, loss = 4.915\n",
      "[batch 272]: seen 341250 words at 14233.0 wps, loss = 4.894\n",
      "[batch 284]: seen 356250 words at 14239.8 wps, loss = 4.873\n",
      "[batch 296]: seen 371250 words at 14242.1 wps, loss = 4.854\n",
      "[batch 308]: seen 386250 words at 14234.7 wps, loss = 4.835\n",
      "[batch 320]: seen 401250 words at 14222.0 wps, loss = 4.817\n",
      "[batch 331]: seen 415000 words at 14203.7 wps, loss = 4.801\n",
      "[batch 343]: seen 430000 words at 14205.4 wps, loss = 4.786\n",
      "[batch 354]: seen 443750 words at 14188.0 wps, loss = 4.772\n",
      "[batch 365]: seen 457500 words at 14160.0 wps, loss = 4.756\n",
      "[batch 376]: seen 471250 words at 14141.3 wps, loss = 4.744\n",
      "[batch 388]: seen 486250 words at 14148.0 wps, loss = 4.732\n",
      "[batch 400]: seen 501250 words at 14160.9 wps, loss = 4.718\n",
      "[batch 412]: seen 516250 words at 14165.7 wps, loss = 4.705\n",
      "[batch 424]: seen 531250 words at 14177.8 wps, loss = 4.692\n",
      "[batch 436]: seen 546250 words at 14189.6 wps, loss = 4.681\n",
      "[batch 448]: seen 561250 words at 14183.0 wps, loss = 4.668\n",
      "[batch 460]: seen 576250 words at 14198.0 wps, loss = 4.656\n",
      "[batch 472]: seen 591250 words at 14197.9 wps, loss = 4.646\n",
      "[batch 484]: seen 606250 words at 14198.6 wps, loss = 4.635\n",
      "[batch 495]: seen 620000 words at 14185.8 wps, loss = 4.625\n",
      "[batch 506]: seen 633750 words at 14172.5 wps, loss = 4.616\n",
      "[batch 517]: seen 647500 words at 14145.9 wps, loss = 4.608\n",
      "[batch 528]: seen 661250 words at 14109.2 wps, loss = 4.599\n",
      "[batch 539]: seen 675000 words at 14081.4 wps, loss = 4.591\n",
      "[batch 550]: seen 688750 words at 14052.4 wps, loss = 4.582\n",
      "[batch 562]: seen 703750 words at 14052.6 wps, loss = 4.574\n",
      "[batch 573]: seen 717500 words at 14044.9 wps, loss = 4.565\n",
      "[batch 585]: seen 732500 words at 14054.4 wps, loss = 4.557\n",
      "[batch 597]: seen 747500 words at 14062.1 wps, loss = 4.548\n",
      "[batch 609]: seen 762500 words at 14059.8 wps, loss = 4.540\n",
      "[batch 620]: seen 776250 words at 14047.4 wps, loss = 4.533\n",
      "[batch 632]: seen 791250 words at 14048.2 wps, loss = 4.525\n",
      "[batch 644]: seen 806250 words at 14052.4 wps, loss = 4.518\n",
      "[batch 656]: seen 821250 words at 14059.7 wps, loss = 4.511\n",
      "[batch 668]: seen 836250 words at 14063.0 wps, loss = 4.504\n",
      "[batch 680]: seen 851250 words at 14068.4 wps, loss = 4.497\n",
      "[batch 692]: seen 866250 words at 14071.9 wps, loss = 4.491\n",
      "[batch 704]: seen 881250 words at 14070.0 wps, loss = 4.484\n",
      "[batch 715]: seen 895000 words at 14061.1 wps, loss = 4.478\n",
      "[batch 727]: seen 910000 words at 14060.9 wps, loss = 4.472\n",
      "[batch 739]: seen 925000 words at 14064.0 wps, loss = 4.466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 751]: seen 940000 words at 14063.0 wps, loss = 4.460\n",
      "[batch 763]: seen 955000 words at 14060.9 wps, loss = 4.454\n",
      "[batch 775]: seen 969900 words at 14058.9 wps, loss = 4.448\n",
      "Train set: avg. loss: 5.224  (perplexity: 185.64)\n",
      "Test set: avg. loss: 5.289  (perplexity: 198.24)\n",
      "[epoch 2] Completed in 0:02:14\n",
      "[epoch 3] Starting epoch 3\n",
      "[batch 11]: seen 15000 words at 14111.0 wps, loss = 7.092\n",
      "[batch 23]: seen 30000 words at 14126.5 wps, loss = 6.578\n",
      "[batch 35]: seen 45000 words at 14246.3 wps, loss = 6.235\n",
      "[batch 47]: seen 60000 words at 14317.5 wps, loss = 5.999\n",
      "[batch 59]: seen 75000 words at 14369.8 wps, loss = 5.818\n",
      "[batch 71]: seen 90000 words at 14363.7 wps, loss = 5.673\n",
      "[batch 83]: seen 105000 words at 14278.5 wps, loss = 5.560\n",
      "[batch 94]: seen 118750 words at 14116.2 wps, loss = 5.477\n",
      "[batch 106]: seen 133750 words at 14152.5 wps, loss = 5.394\n",
      "[batch 118]: seen 148750 words at 14200.4 wps, loss = 5.328\n",
      "[batch 130]: seen 163750 words at 14200.1 wps, loss = 5.276\n",
      "[batch 142]: seen 178750 words at 14211.8 wps, loss = 5.225\n",
      "[batch 154]: seen 193750 words at 14195.2 wps, loss = 5.181\n",
      "[batch 166]: seen 208750 words at 14191.0 wps, loss = 5.141\n",
      "[batch 178]: seen 223750 words at 14193.6 wps, loss = 5.103\n",
      "[batch 190]: seen 238750 words at 14206.7 wps, loss = 5.069\n",
      "[batch 201]: seen 252500 words at 14166.2 wps, loss = 5.042\n",
      "[batch 211]: seen 265000 words at 14043.4 wps, loss = 5.019\n",
      "[batch 221]: seen 277500 words at 13922.0 wps, loss = 4.996\n",
      "[batch 233]: seen 292500 words at 13937.2 wps, loss = 4.970\n",
      "[batch 243]: seen 305000 words at 13820.8 wps, loss = 4.947\n",
      "[batch 248]: seen 311250 words at 13464.0 wps, loss = 4.939\n",
      "[batch 256]: seen 321250 words at 13241.3 wps, loss = 4.924\n",
      "[batch 263]: seen 330000 words at 13006.6 wps, loss = 4.911\n",
      "[batch 270]: seen 338750 words at 12800.9 wps, loss = 4.898\n",
      "[batch 277]: seen 347500 words at 12594.8 wps, loss = 4.887\n",
      "[batch 284]: seen 356250 words at 12398.2 wps, loss = 4.875\n",
      "[batch 291]: seen 365000 words at 12225.9 wps, loss = 4.865\n",
      "[batch 297]: seen 372500 words at 12052.6 wps, loss = 4.855\n",
      "[batch 303]: seen 380000 words at 11885.4 wps, loss = 4.846\n",
      "[batch 309]: seen 387500 words at 11733.2 wps, loss = 4.837\n",
      "[batch 314]: seen 393750 words at 11550.8 wps, loss = 4.830\n",
      "[batch 319]: seen 400000 words at 11375.0 wps, loss = 4.823\n",
      "[batch 324]: seen 406250 words at 11172.5 wps, loss = 4.815\n",
      "[batch 336]: seen 421250 words at 11266.1 wps, loss = 4.798\n",
      "[batch 347]: seen 435000 words at 11320.3 wps, loss = 4.784\n",
      "[batch 358]: seen 448750 words at 11377.9 wps, loss = 4.770\n",
      "[batch 371]: seen 465000 words at 11494.0 wps, loss = 4.755\n",
      "[batch 384]: seen 481250 words at 11607.7 wps, loss = 4.741\n",
      "[batch 397]: seen 497500 words at 11713.0 wps, loss = 4.727\n",
      "[batch 410]: seen 513750 words at 11815.9 wps, loss = 4.714\n",
      "[batch 423]: seen 530000 words at 11914.0 wps, loss = 4.700\n",
      "[batch 436]: seen 546250 words at 12007.6 wps, loss = 4.688\n",
      "[batch 449]: seen 562500 words at 12098.4 wps, loss = 4.674\n",
      "[batch 462]: seen 578750 words at 12185.1 wps, loss = 4.662\n",
      "[batch 475]: seen 595000 words at 12264.4 wps, loss = 4.651\n",
      "[batch 488]: seen 611250 words at 12341.5 wps, loss = 4.639\n",
      "[batch 501]: seen 627500 words at 12413.1 wps, loss = 4.627\n",
      "[batch 514]: seen 643750 words at 12485.6 wps, loss = 4.617\n",
      "[batch 527]: seen 660000 words at 12551.4 wps, loss = 4.607\n",
      "[batch 540]: seen 676250 words at 12619.9 wps, loss = 4.597\n",
      "[batch 553]: seen 692500 words at 12685.0 wps, loss = 4.587\n",
      "[batch 566]: seen 708750 words at 12743.5 wps, loss = 4.577\n",
      "[batch 579]: seen 725000 words at 12797.0 wps, loss = 4.568\n",
      "[batch 592]: seen 741250 words at 12851.8 wps, loss = 4.558\n",
      "[batch 605]: seen 757500 words at 12906.8 wps, loss = 4.549\n",
      "[batch 618]: seen 773750 words at 12960.5 wps, loss = 4.540\n",
      "[batch 632]: seen 791250 words at 13019.3 wps, loss = 4.531\n",
      "[batch 646]: seen 808750 words at 13075.9 wps, loss = 4.522\n",
      "[batch 660]: seen 826250 words at 13131.1 wps, loss = 4.513\n",
      "[batch 674]: seen 843750 words at 13184.3 wps, loss = 4.505\n",
      "[batch 688]: seen 861250 words at 13236.2 wps, loss = 4.497\n",
      "[batch 701]: seen 877500 words at 13281.5 wps, loss = 4.490\n",
      "[batch 715]: seen 895000 words at 13330.0 wps, loss = 4.482\n",
      "[batch 729]: seen 912500 words at 13377.0 wps, loss = 4.475\n",
      "[batch 743]: seen 930000 words at 13422.7 wps, loss = 4.468\n",
      "[batch 757]: seen 947500 words at 13466.3 wps, loss = 4.461\n",
      "[batch 771]: seen 965000 words at 13509.0 wps, loss = 4.454\n",
      "Train set: avg. loss: 5.222  (perplexity: 185.37)\n",
      "Test set: avg. loss: 5.286  (perplexity: 197.47)\n",
      "[epoch 3] Completed in 0:02:14\n",
      "[epoch 4] Starting epoch 4\n",
      "[batch 12]: seen 16250 words at 15148.0 wps, loss = 7.095\n",
      "[batch 25]: seen 32500 words at 15310.8 wps, loss = 6.536\n",
      "[batch 39]: seen 50000 words at 15648.5 wps, loss = 6.150\n",
      "[batch 53]: seen 67500 words at 15824.9 wps, loss = 5.892\n",
      "[batch 66]: seen 83750 words at 15866.1 wps, loss = 5.740\n",
      "[batch 79]: seen 100000 words at 15880.6 wps, loss = 5.607\n",
      "[batch 93]: seen 117500 words at 15959.6 wps, loss = 5.509\n",
      "[batch 107]: seen 135000 words at 16013.8 wps, loss = 5.426\n",
      "[batch 121]: seen 152500 words at 16072.6 wps, loss = 5.361\n",
      "[batch 135]: seen 170000 words at 16102.9 wps, loss = 5.300\n",
      "[batch 149]: seen 187500 words at 16140.0 wps, loss = 5.244\n",
      "[batch 163]: seen 205000 words at 16155.6 wps, loss = 5.195\n",
      "[batch 177]: seen 222500 words at 16165.2 wps, loss = 5.151\n",
      "[batch 191]: seen 240000 words at 16180.4 wps, loss = 5.109\n",
      "[batch 205]: seen 257500 words at 16186.8 wps, loss = 5.072\n",
      "[batch 219]: seen 275000 words at 16198.0 wps, loss = 5.039\n",
      "[batch 233]: seen 292500 words at 16210.3 wps, loss = 5.006\n",
      "[batch 246]: seen 308750 words at 16210.3 wps, loss = 4.979\n",
      "[batch 260]: seen 326250 words at 16220.6 wps, loss = 4.952\n",
      "[batch 274]: seen 343750 words at 16229.2 wps, loss = 4.927\n",
      "[batch 287]: seen 360000 words at 16227.0 wps, loss = 4.904\n",
      "[batch 300]: seen 376250 words at 16225.0 wps, loss = 4.882\n",
      "[batch 314]: seen 393750 words at 16227.0 wps, loss = 4.859\n",
      "[batch 328]: seen 411250 words at 16238.8 wps, loss = 4.838\n",
      "[batch 342]: seen 428750 words at 16246.0 wps, loss = 4.818\n",
      "[batch 355]: seen 445000 words at 16245.3 wps, loss = 4.802\n",
      "[batch 368]: seen 461250 words at 16216.2 wps, loss = 4.784\n",
      "[batch 381]: seen 477500 words at 16180.3 wps, loss = 4.770\n",
      "[batch 393]: seen 492500 words at 16141.2 wps, loss = 4.756\n",
      "[batch 406]: seen 508750 words at 16137.9 wps, loss = 4.743\n",
      "[batch 420]: seen 526250 words at 16141.8 wps, loss = 4.728\n",
      "[batch 434]: seen 543750 words at 16148.0 wps, loss = 4.713\n",
      "[batch 447]: seen 560000 words at 16147.7 wps, loss = 4.699\n",
      "[batch 460]: seen 576250 words at 16144.9 wps, loss = 4.686\n",
      "[batch 473]: seen 592500 words at 16141.2 wps, loss = 4.674\n",
      "[batch 486]: seen 608750 words at 16140.8 wps, loss = 4.661\n",
      "[batch 500]: seen 626250 words at 16145.4 wps, loss = 4.649\n",
      "[batch 513]: seen 642500 words at 16146.8 wps, loss = 4.638\n",
      "[batch 526]: seen 658750 words at 16144.7 wps, loss = 4.628\n",
      "[batch 540]: seen 676250 words at 16151.6 wps, loss = 4.616\n",
      "[batch 554]: seen 693750 words at 16156.2 wps, loss = 4.606\n",
      "[batch 568]: seen 711250 words at 16165.4 wps, loss = 4.595\n",
      "[batch 582]: seen 728750 words at 16173.4 wps, loss = 4.585\n",
      "[batch 596]: seen 746250 words at 16178.3 wps, loss = 4.574\n",
      "[batch 610]: seen 763750 words at 16186.5 wps, loss = 4.565\n",
      "[batch 624]: seen 781250 words at 16192.3 wps, loss = 4.556\n",
      "[batch 638]: seen 798750 words at 16194.8 wps, loss = 4.545\n",
      "[batch 652]: seen 816250 words at 16197.2 wps, loss = 4.536\n",
      "[batch 666]: seen 833750 words at 16201.7 wps, loss = 4.527\n",
      "[batch 679]: seen 850000 words at 16199.0 wps, loss = 4.519\n",
      "[batch 693]: seen 867500 words at 16203.9 wps, loss = 4.510\n",
      "[batch 707]: seen 885000 words at 16209.4 wps, loss = 4.502\n",
      "[batch 721]: seen 902500 words at 16212.7 wps, loss = 4.495\n",
      "[batch 734]: seen 918750 words at 16212.6 wps, loss = 4.487\n",
      "[batch 747]: seen 935000 words at 16206.0 wps, loss = 4.481\n",
      "[batch 760]: seen 951250 words at 16199.7 wps, loss = 4.474\n",
      "[batch 773]: seen 967500 words at 16193.1 wps, loss = 4.467\n",
      "Train set: avg. loss: 5.224  (perplexity: 185.67)\n",
      "Test set: avg. loss: 5.287  (perplexity: 197.70)\n",
      "[epoch 4] Completed in 0:02:03\n",
      "[epoch 5] Starting epoch 5\n",
      "[batch 12]: seen 16250 words at 16150.1 wps, loss = 7.128\n",
      "[batch 26]: seen 33750 words at 16237.2 wps, loss = 6.530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 40]: seen 51250 words at 16255.1 wps, loss = 6.180\n",
      "[batch 54]: seen 68750 words at 16294.5 wps, loss = 5.926\n",
      "[batch 68]: seen 86250 words at 16317.8 wps, loss = 5.740\n",
      "[batch 82]: seen 103750 words at 16314.4 wps, loss = 5.612\n",
      "[batch 96]: seen 121250 words at 16332.1 wps, loss = 5.505\n",
      "[batch 110]: seen 138750 words at 16328.0 wps, loss = 5.416\n",
      "[batch 124]: seen 156250 words at 16339.5 wps, loss = 5.341\n",
      "[batch 138]: seen 173750 words at 16340.0 wps, loss = 5.274\n",
      "[batch 152]: seen 191250 words at 16337.2 wps, loss = 5.218\n",
      "[batch 166]: seen 208750 words at 16331.7 wps, loss = 5.171\n",
      "[batch 180]: seen 226250 words at 16330.7 wps, loss = 5.128\n",
      "[batch 194]: seen 243750 words at 16336.7 wps, loss = 5.086\n",
      "[batch 208]: seen 261250 words at 16345.0 wps, loss = 5.050\n",
      "[batch 221]: seen 277500 words at 16334.0 wps, loss = 5.018\n",
      "[batch 235]: seen 295000 words at 16336.5 wps, loss = 4.986\n",
      "[batch 248]: seen 311250 words at 16330.0 wps, loss = 4.958\n",
      "[batch 261]: seen 327500 words at 16314.8 wps, loss = 4.934\n",
      "[batch 275]: seen 345000 words at 16313.9 wps, loss = 4.910\n",
      "[batch 288]: seen 361250 words at 16296.0 wps, loss = 4.888\n",
      "[batch 301]: seen 377500 words at 16274.8 wps, loss = 4.867\n",
      "[batch 314]: seen 393750 words at 16225.3 wps, loss = 4.847\n",
      "[batch 327]: seen 410000 words at 16222.9 wps, loss = 4.827\n",
      "[batch 341]: seen 427500 words at 16226.8 wps, loss = 4.808\n",
      "[batch 355]: seen 445000 words at 16237.8 wps, loss = 4.791\n",
      "[batch 369]: seen 462500 words at 16242.9 wps, loss = 4.773\n",
      "[batch 382]: seen 478750 words at 16236.3 wps, loss = 4.758\n",
      "[batch 396]: seen 496250 words at 16241.6 wps, loss = 4.742\n",
      "[batch 410]: seen 513750 words at 16245.6 wps, loss = 4.727\n",
      "[batch 423]: seen 530000 words at 16245.5 wps, loss = 4.713\n",
      "[batch 437]: seen 547500 words at 16249.8 wps, loss = 4.698\n",
      "[batch 451]: seen 565000 words at 16255.0 wps, loss = 4.683\n",
      "[batch 464]: seen 581250 words at 16254.2 wps, loss = 4.671\n",
      "[batch 478]: seen 598750 words at 16264.8 wps, loss = 4.658\n",
      "[batch 492]: seen 616250 words at 16274.8 wps, loss = 4.645\n",
      "[batch 506]: seen 633750 words at 16280.9 wps, loss = 4.633\n",
      "[batch 520]: seen 651250 words at 16281.5 wps, loss = 4.622\n",
      "[batch 534]: seen 668750 words at 16285.3 wps, loss = 4.611\n",
      "[batch 548]: seen 686250 words at 16285.0 wps, loss = 4.600\n",
      "[batch 562]: seen 703750 words at 16288.8 wps, loss = 4.589\n",
      "[batch 576]: seen 721250 words at 16296.0 wps, loss = 4.579\n",
      "[batch 590]: seen 738750 words at 16299.2 wps, loss = 4.569\n",
      "[batch 604]: seen 756250 words at 16297.3 wps, loss = 4.559\n",
      "[batch 617]: seen 772500 words at 16295.1 wps, loss = 4.550\n",
      "[batch 631]: seen 790000 words at 16300.0 wps, loss = 4.541\n",
      "[batch 645]: seen 807500 words at 16302.1 wps, loss = 4.531\n",
      "[batch 659]: seen 825000 words at 16304.6 wps, loss = 4.522\n",
      "[batch 673]: seen 842500 words at 16310.9 wps, loss = 4.514\n",
      "[batch 687]: seen 860000 words at 16318.0 wps, loss = 4.506\n",
      "[batch 700]: seen 876250 words at 16310.7 wps, loss = 4.499\n",
      "[batch 713]: seen 892500 words at 16300.4 wps, loss = 4.491\n",
      "[batch 726]: seen 908750 words at 16296.9 wps, loss = 4.484\n",
      "[batch 740]: seen 926250 words at 16298.8 wps, loss = 4.476\n",
      "[batch 754]: seen 943750 words at 16299.1 wps, loss = 4.469\n",
      "[batch 767]: seen 960000 words at 16298.0 wps, loss = 4.462\n",
      "Train set: avg. loss: 5.218  (perplexity: 184.53)\n",
      "Test set: avg. loss: 5.281  (perplexity: 196.66)\n",
      "[epoch 5] Completed in 0:02:02\n",
      "[epoch 5] Train set: avg. loss: 5.218  (perplexity: 184.53)\n",
      "[epoch 5] Test set: avg. loss: 5.281  (perplexity: 196.66)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        \n",
    "        tf.set_random_seed(42)\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        run_epoch(lm, session, bi, learning_rate=learning_rate,\n",
    "                       train=True, verbose=True, tick_s=1.0)\n",
    "        train_loss = score_dataset(lm, session, train_ids,\n",
    "                                        name=\"Train set\")\n",
    "        test_loss = score_dataset(lm, session, test_ids,\n",
    "                                       name=\"Test set\")\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "    ##\n",
    "    # score_dataset will run a forward pass over the entire dataset\n",
    "    # and report perplexity scores. This can be slow (around 1/2 to \n",
    "    # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "    # to speed up training on a slow machine. Be sure to run it at the \n",
    "    # end to evaluate your score.\n",
    "    print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "    score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "    print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "    score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Sampling Sentences (5 points)\n",
    "\n",
    "If you didn't already in **part (b)**, implement the `BuildSamplerGraph()` method in `rnnlm.py` See the function docstring for more information.\n",
    "\n",
    "#### Implement the `sample_step()` method below (5 points)\n",
    "This should access the Tensors you create in `BuildSamplerGraph()`. Given an input batch and initial states, it should return a vector of shape `[batch_size,1]` containing sampled indices for the next word of each batch sequence.\n",
    "\n",
    "Run the method using the provided code to generate 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    # Run sample ops\n",
    "    feed_dict = {\n",
    "        lm.input_w_: input_w,\n",
    "        lm.initial_h_: initial_h\n",
    "    }\n",
    "    ops = [lm.final_h_, lm.pred_samples_]        \n",
    "    final_h, samples = session.run(ops, feed_dict=feed_dict)\n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "<s> and sociological the columns orders has sought , survivors her can't be studied , he went to her . <s> \n",
      "<s> entirely to get <unk> minerals in the federal atoms appointed antagonism or similar <unk> alone for as a preparation , \n",
      "<s> how the <unk> state are spreads with the traditional withheld and <unk> stomach . <s> \n",
      "<s> but it's my hold me and independently , from example . <s> \n",
      "<s> and resource in interpretations which had made an general comes . <s> \n",
      "<s> and if kennedy who shared hopeful , at the following stream of at the stranger of president , factories as \n",
      "<s> -- when that not she felt the t the policy , . <s> \n",
      "<s> they were way to get up the minority night of our was all noise . <s> \n",
      "<s> any finished made his walton , he had only him to see the barker of required . <s> \n",
      "<s> he was the <unk> reference . <s> \n"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    # Make initial state for a batch with batch_size = num_samples\n",
    "    w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    # We'll take one step for each sequence on each iteration \n",
    "    for i in range(max_steps):\n",
    "        h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "        w = np.hstack((w,y))\n",
    "\n",
    "    # Print generated sentences\n",
    "    for row in w:\n",
    "        for i, word_id in enumerate(row):\n",
    "            print(vocab.id_to_word[word_id], end=\" \")\n",
    "            if (i != 0) and (word_id == vocab.START_ID):\n",
    "                break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (e) Linguistic Properties (5 points)\n",
    "\n",
    "Now that we've trained our RNNLM, let's test a few properties of the model to see how well it learns linguistic phenomena. We'll do this with a scoring task: given two or more test sentences, our model should score the more plausible (or more correct) sentence with a higher log-probability.\n",
    "\n",
    "We'll define a scoring function to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"once upon a time\" : -7.09\n",
      "\"the quick brown fox jumps over the lazy dog\" : -7.47\n"
     ]
    }
   ],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number agreement\n",
    "\n",
    "Compare **\"the boy and the girl [are/is]\"**. Which is more plausible according to your model?\n",
    "\n",
    "If your model doesn't order them correctly (*this is OK*), why do you think that might be? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"the boy and the girl are\" : -5.53\n",
      "\"the boy and the girl is\" : -5.48\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "sents = [\"the boy and the girl are\",\n",
    "         \"the boy and the girl is\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities are very close, but \"is\" wins over \"are\" (\"are\" is grammatically correct). This might be because the RNN was trained on more examples with \"is\" following \"and the girl\" than \"are\". There might be less examples in the training set with nouns combined by \"and\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Type/semantic agreement\n",
    "\n",
    "Compare:\n",
    "- **\"peanuts are my favorite kind of [nut/vegetable]\"**\n",
    "- **\"when I'm hungry I really prefer to [eat/drink]\"**\n",
    "\n",
    "Of each pair, which is more plausible according to your model?\n",
    "\n",
    "How would you expect a 3-gram language model to perform at this example? How about a 5-gram model? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"peanuts are my favorite kind of nut\" : -7.13\n",
      "\"peanuts are my favorite kind of vegetable\" : -6.97\n",
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"when I'm hungry I really prefer to eat\" : -7.17\n",
      "\"when I'm hungry I really prefer to drink\" : -7.49\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "sents = [\"peanuts are my favorite kind of nut\",\n",
    "         \"peanuts are my favorite kind of vegetable\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "sents = [\"when I'm hungry I really prefer to eat\",\n",
    "         \"when I'm hungry I really prefer to drink\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peanuts: the vegetable option is more plausible according to the model. This might be because there are more examples in the training data of folks commenting on their favorite vegetable.\n",
    "\n",
    "Hungry: the eat option is more plausible according to the model. This is correct.\n",
    "\n",
    "I would expect the 3-gram and 5-gram models to perform worse at these examples, as in both the key token (\"peanuts\" in the first and \"hungry\" in the second) are outside of the context window of 3- and 5-gram models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adjective ordering (just for fun)\n",
    "\n",
    "Let's repeat the exercise from Week 2:\n",
    "\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "We'll consider a toy example (literally), and consider all possible adjective permutations.\n",
    "\n",
    "Note that this is somewhat sensitive to training, and even a good language model might not get it all correct. Why might the NN fail, if the trigram model from Week 2 was able to solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"I have lots of green square plastic toys\" : -8.00\n",
      "\"I have lots of green plastic square toys\" : -8.00\n",
      "\"I have lots of plastic green square toys\" : -8.00\n",
      "\"I have lots of plastic square green toys\" : -8.10\n",
      "\"I have lots of square plastic green toys\" : -8.15\n",
      "\"I have lots of square green plastic toys\" : -8.19\n"
     ]
    }
   ],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "    words = prefix + list(adjs) + [noun]\n",
    "    inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the ordering example from Week 2, we would expect the proper ordering to be \"shape-color-material\" or \"square green plastic\".\n",
    "\n",
    "First you have to consider that the it is rare for three adjectives to be strung together in everyday English. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
